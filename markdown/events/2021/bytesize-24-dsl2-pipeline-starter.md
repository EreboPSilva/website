---
title: 'Bytesize 24: Where do I start writing my own DSL2 pipeline?!'
subtitle: Harshil Patel - Seqera Labs, Spain
type: talk
start_date: '2021-10-19'
start_time: '13:00 CEST'
end_date: '2021-10-19'
end_time: '13:30 CEST'
youtube_embed: https://youtu.be/Z_uPj7fAes8
location_url:
  - https://youtu.be/Z_uPj7fAes8
  - https://www.bilibili.com/video/BV1nq4y197FR
  - https://doi.org/10.6084/m9.figshare.16836616.v1
---

# nf-core/bytesize

Join us for an episode of our **weekly series** of short talks: **“nf-core/bytesize”**.

Just **15 minutes** + questions, we will be focussing on topics about using and developing nf-core pipelines.
These will be recorded and made available at <https://nf-co.re>
It is our hope that these talks / videos will build an archive of training material that can complement our documentation. Got an idea for a talk? Let us know on the [`#bytesize`](https://nfcore.slack.com/channels/bytesize) Slack channel!

## Bytesize 24: Where do you begin? DSL2 pipeline introduction

This week, Harshil Patel ([@drpatelh](http://github.com/drpatelh/)) will present an introduction to developing pipelines in Nextflow DSL2 using nf-core community standards.

Slides:

<div class="ratio ratio-16x9">
    <iframe src="https://widgets.figshare.com/articles/16836616/embed?show_title=1" width="568" height="351" allowfullscreen frameborder="0"></iframe>
</div>

<details markdown="1"><summary>Video transcription</summary>
**Note: The content has been edited for reader-friendliness**

[0:01](https://youtu.be/Z_uPj7fAes8&t=1)
[:](https://youtu.be/Z_uPj7fAes8&t=)
talk which is in preparation for the big DSL2 hackathon next week and today we have Harshil Patel from Secura Labs presenting about writing your own nf quad DSL2 pipeline. During the talk if you have any questions for Harshal please put them in the chat and I'll read them out. As a reminder this video and all the previous bite-sized talks are on the YouTube channel so you can consult them there later as well and all the links are on our homepage and so please take it away Harshal. Hello everyone good afternoon and thank you for watching and joining this talk. So I will be giving the 24th bite-sized nf quad bite-sized talk. It's just amazing how we have got this far with this many talks now so I remember when we were still setting this stuff up but hopefully they're useful to you guys. I mean this particular one will be about um you know trying to deal with the exploding head that is I've got a DSL1 pipeline or I'm comfortable with DSL1 how do I now switch to to writing a DSL2 next flow pipeline. So next flow as some of you know has this new syntax called DSL2 which which is a more modular syntax it allows you to reuse components and and essentially make things a bit more flexible in terms of pipeline development as well. So there's there's loads of resources so as I mentioned we have bite-sized talks that are you know sort of revolved around DSL2 pipeline development how we are tackling DSL2 on nf core but there are some generic concepts there that may be useful to you as well if you like in in the bite-sized tests and also some guidelines and and other stuff that you may be able to take away to use for your own DSL2 pipeline development in say non-nf4 pipelines too. Last week uh Rike Maxim and I recorded a couple of talks more up-to-date talks about the pipeline structure that we have for DSL2 um in in the template that we have we maintain on nf4 tools and I just briefly went through that again in preparation for the hackathon so people get an idea as to what these files are doing and why they're there and another talk which was typically a 45-minute talk by my standards that that is where I kind of get with these talks and that was more about the process of picking a module and the using the various tooling that we've created to then contribute that back to nf core modules but again there are some generic concepts there that may be useful to you um to take away. So the first thing um I would do is potentially attempt to watch some of these talks here um and also we've got loads of documentation and guidelines on the website that'd be worth looking at I mean um figuring out how to tackle DSL2 in some way philosophically I guess would be um in comparison to the way that you would attempt to figure out how these components are put together and the the smallest unit of a DSL2 pipeline will be a module and so attempting to figure out how you need to mentally create and write these modules to structure um an entire pipeline together is quite useful and so we've got these guidelines and adding new modules and some of the concepts that we've used to attempt to standardize these these modules and the syntax available on the website for you to to have a look at. How you how you convert your pipeline will obviously depend on on what status it is and um you know where you're coming from where you're starting from so existing nf core pipelines a lot of the groundwork has now been done in terms of um porting our old DSL1 template to a DSL2 template and you know switching things around adding tests all the boilerplate and this is one of the big advantages actually of you know working in a community like this because it's not it's not just me or another person or someone else doing this is there's an entire community contributing to this and so it just makes it easier to maintain and push these changes and across an entire sway the pipelines so those starting out with existing pipelines you just would need to merge in the template sync that you would have got via a pull request and this is automated it's it's it's sent out when whenever we release the nf core tools package um and you would have a pull request there that is sitting there waiting to be merged in so the first thing i would do with that is just merge that in you may have quite a few merge conflicts but unfortunately because this is such a big change you will have to wrestle with those for now um and and hopefully in the future they'll get smaller and smaller as things stabilize get the tests working again so with nf core pipelines we insist on having test data sets for for continuous integration and for local testing it just means that whenever you update the pipeline code you can test the pipeline to make sure it's working so after you've merged in the the the the template sync try and get the test working again even if it's still in a dsl1 format just try and get the test working again and then decide how you want to tackle the implementation which i'll go through briefly in some of the following slides uh with new nf core pipelines uh we have again loads of guidelines and docs there's there's conceptually there's there's there's actually very small differences in the way that the template has changed between dsl1 and dsl2 um in order to manage the modules aspect there are more changes actually probably have been in the way that we've siloed away some of the boilerplate code into into lib directories to to make it easier to um to actually read the code that we've got there and to update it so um again more guidelines and documentations on the website i won't go through that in detail have a look um but most importantly if you want to contribute a pipeline to nfq you're thinking of doing it whether you know you start off uh by you know with the new nfq pipeline or you you start off thinking i'm just going to write my own pipeline but maybe i might contribute to nfq in the future please come and approach us first because we we try not to have redundancy in pipelines and um it's always sort of a community decision as to what gets in um and so yeah it'd be great if you can approach this before you lay down in your code i mean it things can become a bit awkward when you've written a pipeline and it's fully functional and it's in its all you know in its awesomeness um but it may not necessarily fit with what we have or what we require and so in that case we don't want to disappoint you or you know or just make things difficult for everyone if you've got a non-nfq pipeline uh you know so so for example you don't want to contribute to nfq you can still use the tooling that we've created um it's completely up to you and it's flexible how you adopt the standards and the template and the different files and stuff that we've got in the template this is simply done by using the nfq create command in the nfq tools package it's one command you create you get a bunch of boilerplate stuff that you don't have to do yourself if you're just looking at you know writing simple next one pipelines this may be overkill but um you know if you're seriously thinking about writing your own pipeline even to use as a reference to see how how you know the community itself is adopting best practices how they you know running github actions because in this integration configuration linting all sorts of other stuff as well this is very useful to have a look and like i mentioned i gave a talk about that last week so so you can you can see what it looks like and what the files in that repository are doing it also means that you can sync in the the template so when whenever we do a release of nfq tools um all nfq pipelines automatically get this sync pr so anything we've updated in the pipeline template then gets pushed automatically to these pipelines via this sync pr um and if you have created um a pipeline template albeit whether you're not going to not going to contribute to nfq um then you can use the sync functionality to update your template too with that um so it just it just allows you to keep up to date with with the best practices and other boilerplate and bugs fixes and stuff that the community is implementing and it also means the pipeline can be contributed later like i said i mean approaches first if you if you are seriously thinking about it but um if with when you use nfq create it does a few things especially with git there's a bit of magic there that allows you to then um contribute that pipeline to nfq later on down the line if you so wish to so so there's there's another advantage and we also have loads of other nfq tools commands check it out i won't go through them now but there's you know various tools for linting and and um and other and other stuff that that'll be useful for um maintaining and developing the pipelines uh the first port call i would recommend is probably to look at nfq modules that's our repository for um wrapper scripts essentially or modules dslg modules um it's been it's been you know developing immensely well uh we've got you know six to seven contributors we've almost got to 300 modules now which after the hackathon i imagine will completely surpass that um and it's it's just a repository for for standardized module wrapper scripts for individual tools like fastqc or trim galore that you can just pull and use directly in your pipelines you don't need to go through the effort of writing these modules and it saves a lot of work and this kind of fits in with the ethos of of nextflow dsl2 as well um it's constantly evolving i won't say that it's completely stable because it's not um i would say however that we're constantly making it better um and trying to shift towards using um as nextflow nextflow-esque language and approaches as possible so check it out and and to add to that we've got loads of tools that we've added in nfq tools specifically to deal with modules i've listed them here where you can list modules install them update and all sorts of other functionalities some of this i refer to in in the other pre-hackathon talk about contributing to nfq modules um that i gave last week the link was in the first slide and also we plan to hold sub sub workflows in the future so for those of you that don't know what sub workflow is it is a essentially a chain of modules so a module is is a unit of dsl2 let's say where you've got FastQC that runs on a single sample and and performs a particular task however you can chain these together so you can run FastQC and adapt to trimming for example after that as as a sub workflow and and then you get a larger chain of modules that you can then just plug into a pipeline without having to individually chain them together and that i think this this is the true power of dsl2 and how we make sub workflow shareable and reusable across pipelines um is going to be um it's going to be a real test because we'll have to figure out a few other things but hopefully we again we plan to tackle some of this at the hackathon next week and maybe as you can see we've got an nfq modules command we'll probably have an nfq sub workflows command that will install all of the module dependencies as well as the sub workflow wherever it needs to be installed and so all you really have to do is just include it in your pipeline um so getting started uh i would probably start by looking at existing pipelines that have done this i i'm pointing to nfq pipelines here because um it's it's what we know it's what we've done there's there's a full list um if you click on that link there there's a dsl2 tab in on the pipeline health page on the website that allows you to to look at other example pipelines if they're more applicable to you um but i think most importantly is setting up a nice test data set we try and tackle that right at the beginning because it's always good to test your pipeline right from from the from the offset and it also means that other people can collaborate on the pipeline with you um and you can identify bugs and issues and pull requests or locally that that you can fix before um or whilst developing the pipeline together so i would say it's incredibly vital to have a nice minimal test data set that you can use um and also this becomes important when you you know your your your people just want to test the pipeline on their own infrastructures for example um this minimal test data set is independent of the samples they're using and so you know the test data sets should be working and so it allows you to rule out other issues with with infrastructure and such when using next one compiling list of modules this is this is very obviously quite an obvious point but you know you need to know what modules you have we've got loads on nfl modules like i said we've got already got about 300 of them so a lot of this work has probably already been done for you um that's not to say you know we wouldn't like your contributions there too because then it just means it's done for someone else as well and so you know hopefully at some point we'll you know we'll get to a point where a majority majority at the moment is quite genomics focused but um hopefully we'll be getting other modules modules in there from other life science areas as well um find and recycle sub-workflows like i said we're still working on this or adding this to nfq modules or having a separate repository maybe nfq sub-workflows for this um but these are at the moment within pipelines like rnac because the sub-workflows folder that you can have a look in and see if there's any you'd like to reuse from there or from anywhere else um so at the moment it's a manual process but hopefully we'll automate this in the future how you do this um and collaborate on on these modules depends entirely on on on how you want to develop the pipeline so you could create a list of modules as separate issues and then you know work your way through those or you could create a project board um like saric has done um which i will show you in the next slide um or maybe the side after and then you can collaborate and tick off tick these off the list eventually whilst you're developing the pipeline in terms of implementation i think it's um we've built like i said we've built a lot of these tools um so nfq modules create is an is an example of this it just takes a vanilla module template with loads of to-do statements and other things in there that are really useful for newbies and beginners and also just as a reminder to make sure that you filled in the correct bits in the file and so it has a load of to-do statements within this particular template when you run nfq modules create it just replaces the name of the module that you'd like to create within this template and so then you have to go about then replacing the bits you want um in order to finesse and add your your module or create a module um some of this stuff again i went through in that contributing to nfq modules talk so please do have a look at that and you'll get an idea as to as to how that can that can work and you can do that for both local modules ones you don't want to contribute to any module or ones that you do reuse biocontainers the biocontainers are essentially con bioconda packages built within both singularity and docker containers um it's it's an awesome resource that we've been using almost exclusively for for all of our modules and it just means that you can you get a docker container and a singularity container for free we don't have to maintain anything if someone adds a new bioconda package we get that as a container for free and so reusing this um is is is nice because then it just gives you this option and of you know not having to host and maintain this this yourselves so passing sample information around is also quite important um you need to figure out the flow of your pipeline typically what you would do is have you'd have different values in a channel for different sample attributes but this gets a bit complicated when you want to when you want to generalize a module and so the best way to do that is to put all of this sample information into what's called what we have called a meta map and so then you can have as many it's like a python dictionary you can have as many um attributes within there and pass that through a pipeline and that also means you can reuse existing modules and nfo modules and so on and still have access to that meta within your pipeline context so how you do that you use also something you need to think about try and stick to a single syntax convention so we obviously have our own but whatever you do whether you want to develop your own just stick to a single syntax convention because it just makes things consistently easier to maintain and to update over time when you change that syntax or that convection convention um there's enough you know you can reuse what we've done a lot of people are um and the caveat there is that it's constantly evolving um so you know it's it's something you have to keep on top of but i don't think that's a bad thing personally because everything changes everything evolves and please write your modules in a way that they can be reused that's the true power of all of this and that just means that someone you know either whether you contribute them to nfo modules or whether you write them and keep them locally within your pipeline it just means other people can just pull that module straight away and reuse that reuse it without having to do much there are various different approaches um so in terms of how you tackle the implementation i particularly prefer the bottom-up approach where um you you have your main script it's just completely dsl one you essentially just comment out the whole thing and start adding one by one each of these modules into the pipeline it just allows you to test every step of the way um and also there's there's other things around the way that you pass channels and and and manipulate channels to these modules that it allows you to do quite interactively whilst you're developing the pipeline and this is what what i prefer i'm doing but there are other approaches so for example there's a couple of links to issues there where we've been creating a list of modules that you can see on the right here and that we worked our way through and this can like i said the individual issues or you can create a project board which is what that and of course eric link there will take you to you can do a top-down approach where you write your modules first and then stitch the pipeline together um there may be caveats in the way that you do that it's not impossible um so um i think praveen done that and maxine done that with nf4 our neighbour um where they wrote the modules and then stitch the pipelines together and there are a couple of caveats in terms of the way that you may need to update modules whilst you're then developing the pipeline because you've already written them and so you may need to change them to fit into the pipeline and stuff um but it's still a valid and plausible approach um we will be changing the syntax that we're using for dsl2 very soon hopefully we're moving to a more um next to a native syntax and i've provided a brief description of this in the contributors to modules talk um at that particular time um if you want to skip through to it so i won't go through this in in any detail but the information is there and hopefully what this means is it will make um it will make the adoption and usage of these modules even more widely accessible because we'd be using a native next row syntax and that that just removes things like the functions file and other things that you know have been a bit of an issue in terms of customization so yeah watch this space things are evolving but again everything will be updated and and hopefully you'll be able to keep on top so this is something again that we'll probably be discussing and trying to iron out like that um if you need to get in touch slack there's the modules channel on there we have a dsl2 pipelines channel somewhere as well i can't remember what it's called now james will probably tell you when you finish um but we've got we've got another channel for those that are dsl2 conversion channel or something like that for people that are interested in knowing more about the conversion process github twitter youtube um you know reach out however and join the community join the slack workspace there's a lot of information to be gained there and it's and it's ridiculously easy to join so um you know even if you're just lurking in the background you'll be surprised how you can just sieve up the knowledge um thank you to everyone in both communities nf core next flow bioconda um biocontainers my new workload sakira labs an awesome bunch of people and that i will be seeing in mayorka tomorrow um please don't jim said and we have a hand hackathon next uh on the 27th 29th which has partly um been mentioned a few times now uh so if you haven't signed up i think the sign up is still open um but i look forward to seeing you there thank you thank you very much so yeah the channel you were talking to talking about is uh slack channel is dsl2 transition there you go that's it so are there any questions you need to post them on zoom or the slack chat anything well i guess that's it for today um of course we have the hackathon next week and that will be in gathertown as well so it'd be a very nice uh interactive environment so i'm sure you can come by and ask our questions then but otherwise we do have a bite-sized talk uh next week from daniel straub talking about nfq ampli seek this will be normal time one o'clock cet on tuesdays and then like kashal said we have the hackathon wednesday to friday next week um

</details>
