---
title: 'Bytesize: Converting Python scripts into packages for PyPI, Bioconda & Biocontainers'
subtitle: Phil Ewels - Seqera Labs
type: talk
start_date: '2023-05-02'
start_time: '13:00 CEST'
end_date: '2023-05-02'
end_time: '13:30 CEST'
youtube_embed: https://www.youtube.com/watch?v=hOuS6mXCwhk
location_url:
  - https://www.youtube.com/watch?v=hOuS6mXCwhk
---

# nf-core/bytesize

Join us for our **weekly series** of short talks: **‚Äúnf-core/bytesize‚Äù**.

Just **15 minutes** + questions, we will be focussing on topics about using and developing nf-core pipelines.
These will be recorded and made available at <https://nf-co.re>
It is our hope that these talks / videos will build an archive of training material that can complement our documentation. Got an idea for a talk? Let us know on the [`#bytesize`](https://nfcore.slack.com/channels/bytesize) Slack channel!

## Bytesize: Converting Python scripts into packages for PyPI, Bioconda & Biocontainers

This week, Phil Ewels ([@ewels](https://github.com/ewels/)) will show you how to take a Python script and turn it into a stand-alone command-line tool, ready for distribution via the [Python Package Index](https://pypi.org/) (PyPI).

> You can download a `.zip` file of the "before" and "after" code examples Phil demoed [here](/assets/markdown_assets/events/2023/bytesize-python-packaging/python-packaging.zip).

This is a good thing to do for a few reasons:

- More people can use your scripts - not just within Nextflow
  - This is useful for development, for stand-alone testing
  - It's useful for people using other workflow managers
  - It helps when users are testing a method / debugging with small sample sizes
- It allows scripts to be released under different licenses to the pipeline itself
- Software packaging, that is providing container images with all requirements, is handled automatically

Even if it's a small script that you think no-one will ever use outside of your pipeline, it's easy to do and you don't lose anything üôÇ

Once released in PyPI, releases via [Bioconda](https://bioconda.github.io/) are simple (see [Bytesize 40: Software packaging](https://nf-co.re/events/2022/bytesize-40-software-packaging)).
Once in BioConda, software will be available for Conda users, but also Docker + Singularity, via the [BioContainers](https://biocontainers.pro/) project.

<details markdown="1"><summary>Video transcription</summary>
**Note: The content has been edited for reader-friendliness**

[0:01](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=1)
Hello, everyone, and welcome to this week's bytesize yalk. I'm very happy to have Phil here, who is talking today about converting Python scripts into packages for PyPy, Bioconda, and biocontainers. It's your stage, Phil.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
Thank you. Hi, everybody. Thank you for joining me today. I'm going to have a little bit of fun together, hopefully. Today's talk was inspired by a conversation that's come up a few times within nf-core, which is when people have got scripts within a pipeline, so typically within a bin directory, or it could be within the exact shell block of a process. Instead of bundling that script with the pipeline, we instead prefer to package that script or set of scripts as a standalone software package instead. There are a few different reasons why we like to do this. Firstly, it makes the package and the analysis scripts available to anyone to use, even if they're not using Nextflow and not using this pipeline, so that's the greater good of a community, the more reusability, more visibility. It can sometimes help with licensing because we're no longer bundling and modifying code under potentially a different license within the nf-core repo, so the nf-core repo can be MIT and can just call this external tool. It also helps with software packaging, as Fran mentioned. For free, then we get a Docker image, singularity image, a Conda package with all of the different requirements that you might need, so you don't need to spend a lot of time thinking about all the different, setting up custom Docker images and all this stuff. You just package your own scripts as its own standalone tool and you get all of that stuff that could be so much better and all the maintenance can sit alongside the pipeline rather than integrated into the pipeline. It's a nice thing to do and for me, the main reason is that first one, which is that it makes the tool more usable for anyone, not necessarily tied to running within Nextflow, which I think is great because it's nice to use tools on a small scale and then to scale up to using a full size pipeline when you need it.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
I've told people in the past that this is easy, which it is, if you've done it lots of times before, but I thought it's probably time to put my money where my mouth is and actually show the process and hopefully convince you, too, that it isn't so bad. Now a few things to note before I kick off, firstly, I'm going to live code this. I have run through it earlier, so I've got a finished example on my side, which you can't see, which I will copy and paste from occasionally and hopefully refer to, if everything really goes wrong, but in the words of SpaceX, excitement is guaranteed because something will blow up at some point. So join me on that. Secondly, there are many, many ways to do this. My way is not necessarily the same as what I'm going to show and there are better ways to do things and probably recommendations that you should listen to from other people that are much better than mine. My aim today is to try and show you the easiest way to go from Python scripts to something on Biaconda, and I want to try and make that big and friendly and as bytesized as possible.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
Let's start by sharing my screen up here and we will kick off, spotlight my screen for everybody, so hopefully you can still see my face. Yeah, so to start off with a famous XKCD comic about Python environments, which are famously complicated packaging environments, so we're going into something which is known for being difficult and varied, but that's fine, I'm going to keep it as simple as possible and you don't need to worry about all this stuff. I've got a little toy Python script here, it doesn't do very much, it just makes a plot and I wanted some input, so it takes a text file here, delete that now, called `titl.txt` with some text in it, reads that file in, sets it as a variable and sets the plot title to whatever it found and then it saves it. This is our starting point, I can try and run this now. If I do `python analysis.py`, there we go, we've got our plot and my nice plot, so it works, first step. This is where I'm assuming you're starting off, is you have a Python script which works.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
We have a few objectives to do, to take this script into a standalone Python package. Firstly we want to, as far as possible, make things **optional** and **variable**, so instead of having a fixed file name with a string like this, we want a better way to pass this information in to the tool, so we want to build the command line tool. We want to make it available ideally anywhere on the command line on the path, so make it into a proper command line tool rather than a script which you have to call using Python. We can call it "my_analysis_tool" or whatever and run that wherever. Once we've done all that stuff we want to package it up using Python packaging so that we have everything we need to push this package onto the Python package index, and we're going to focus on that. Once we've got this as a tool on PyPy, where anyone can install it, then the steps from PyPy to Conda is fairly easy. Once it's on Conda you get by containers for free which is the Docker image and the Singularity image. Really our destination for today is just Python packaging, just the PyPy. There's another talk, it's fairly old by now, but it's still totally valid, by Alex Peltzer on nf-core bytesize. It takes you from that BioConda packaging steps, so you can follow on this this talk with that one. Hopefully that makes sense.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
First steps first let's try and make this into a command line tool. Now there are a bunch of different ways to do this, probably the classic Python library to do command line parsing is called argpass which many of you may be familiar with. Personally I've tended to use another package called "click", and more recently I am tending to use a package called ""typer"" which is actually based on "click". If I just use the right browser, this is URL, "typer"."Typer", gosh it's quite big, on a bigger screen it looks, I'll just make my window bigger just for a second so not reading anything here but just seeing what the website really looks like. It's got a really good website, it explains a lot about how to use it and you can click through the tutorial here and it tells you about everything, what's happening, why it works and the way it does and how to build something. We can start off with this, the simplest example, and we're going to say import "typer" here. Go up to the top, import "typer", wrap our code in a function name. I can't copy from the VS code browser apparently, so I'm going to indent all of this code. Then I'm going to copy in that last bit which was there... my window... down at the bottom.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
What's happening here, I'm importing a Python library called "typer", which is what we're using for the command line tool, I've put everything into a function which is just called def main and then at the bottom I've said if name equals main, so this is telling Python if this script is run directly, use "typer" to run this function. If I save that, now I can do Python analysis and nothing will happen, it should just work exactly the same, but I can do Python analysis help and you can see we're starting to get a command line tool come in here.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
Next up let's get rid of this file, we don't really care about it being in the file, that was just a convenience, so I'm going to say let's instead pass the title as a command line option. With "typer" we just do that by adding a function argument to this function and I can get rid of this bit completely. To prove it I'll delete that file as well. Let's try again, do `python analysis --help` and sure enough now we have some help text saying hey they are expecting a title which is text and we have no default and if I try and run it without any arguments it will give me a nice error message. Now if I say "hello" there, it's passed into there and our plot has a different title. Okay so that is our first step complete, we have a rudimentary command line interface and we have got rid of that file and we've now got command line options which makes it a much more usable flexible tool and that was not a lot of code I think you'll agree with me. With "typer" you can do many more things. You can obviously add lots more arguments here. You can say it should be an integer or boolean and it will craft the command line for you. You can use options instead of arguments so `--whatever`. You can set defaults, you can write help texts, loads of stuff like that. As you your tool becomes more advanced, maybe you dig into the type of documentation a little bit and learn about how to do that, but that's beyond the scope of today's talk.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
Next up, let's think about how to make this into an installable package and something we can run on the command line anywhere, those two things go together. If someone else comes and wants to run this package they're going to need to be able to import these same python packages, so I'm going to start off by making a new file called "requirements.txt" and I'm going to take these package names there and just pop them in there. We'll come back and use that in a minute and in the short term, if someone wanted to, they could now do pip install minus our requirements.txt and that would install all the requirements for this tool. I'm also going to start moving stuff into some subdirectories and by convention I'm going to put it into a directory called "source". But it doesn't really matter, you can call it whatever you want. I'm going to call it "my_tool" and I'm going to move that python file up into that directory there. I'm also going to create a new file called `__init` and `__.py`. This is a weird looking file name and it's a special case. By doing this in python, it tells the python packaging system that this folder's directory behaves as a python module, which is what we want to install later and so I can write add a docstring at the top saying "my_amazing_tool". I'm actually going to not put anything in here for now apart from a single variable which I'd put here by convention, but really you can do whatever you want. I'm going to call it again, use dunder so double underscore version double underscore and we'll say minus, you know, semantic versioning 0.0.0.1 dev. We'll come back and use this variable a bit later, but for now it doesn't do anything.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
What else? We want to make the type of example slightly more complicated. We're gonna now create a type of app like this. We're going to get rid of this bit at the bottom, because we don't actually need that anymore if we're not going to be running it as a script. We're not going to be calling that python file directly. Get rid of that. We're going to now use a python decorator called "appcommand" here, to tell "typer" that this is a command now to be used within the command line interface. This is a normal secondary set, but a first very simple example is so simple that you almost never use that with "typer". This is what you always do and then you can have multiple functions here decorated with command and you can have multiple sub commands within your CLI, using that way and groups of sub commands and all kinds of things. With nf-core we have grouped sub-commands. You do `nf-core module updates` for example and those are separate sub commands, so that's how you do it here. But for now, this would work in exactly the same way as the example I showed you a second ago.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
I'm going to add, because this is going to be a python package, it's really important to tell everybody about how to use it. I'm going to create a new license file. I am a fan of MIT, so I'm going to make it the MIT license and just paste in the text there that I've grabbed off the web and I'm going to make a readme file, because this is going to turn up on github. We want people to know about what the tool is and how to use it, when they see the repo.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
Okay hopefully you're with me, that's all the simple stuff. Now we'll get on to a slightly more complicated bit about how to take this and make it installable. This is one of the bits where it gets very variable about how you can do it. Typically within python you can use a range of different installable python packages to do your python packaging. It's quite meta. There's a very old one called "distutils" which you shouldn't use and there's one called "setup_tools" which is most common. That's what I'm going to use today. Other people like packaging setups such as one popular one called poetry. There are quite a lot of them so if you have a preference, great, go for it. Maybe in the discussion afterwards people can suggest their favorites, but for now I'm going to stick with setup tools and I'm going to say "setup.py", which again this gets a bit confusing, but you don't necessarily need and "setup.cfg". I should dump in here you don't need to remember how to do this. I don't remember how to do this. I don't think anyone really remembers how to do this. If I do some browsing, type in "setuptools.py.io", you can see there's quite good docs on this website for setup tools. They tell you how to do everything, they talk through it's quite easy to read and they also talk through all the different options of how to build this stuff. You can do it with what's called a "pyproject.toml" file, which is probably what I'll start doing soon when it becomes slightly more standard. There's a setup.cfg file, which is what I'm going to do now and there's also some documentation about the old school way of doing it which is "setup.py". Tor now the "setup.py" file is just for backwards compatibility.

[:](https://www.youtube.com/watch?v=hOuS6mXCwhk&t=)
I'm going to do exactly what it tells me to do here. I'm going to say report setup tools setup save and then I just forget about this file and never look at it again then everything else goes into this setup.Cfg file and you can work through the examples here for now I'm going to cheat for the sake of time and copy in here's one I did earlier and just walk you through what these keys are quickly again I always copy this from the last project I did but you can copy it from the web very easily name is important version is important because when you're updating a python package it needs to know which version number it is and this then is using the special variable I set up here now if you look where it is it's in the python module I made called my tool and it's.the variable number is underscore underscore version and so here I'm saying use an attribute I could hard code it in this file if I wanted to and I'm using it as an attribute and I'm using this variable which is under my tool underscore underscore version you could call that whatever you want or you could just hard code it in this file author description keywords license license files long descriptions say it's marked down that's just what shows on the PyPy website classifiers which are just categories I always copy these without thinking you can probably think a bit more about it if you want to and then some some slightly more interesting stuff that on here the minimum required version of python which might be important for you where you put your source code in this case I say look for any packages you can find any python modules you can find and look in the directory called source so if you call that something different you put that here and then that's looking for.init files like that and then saying we require but by a bunch of other python packages here and here I'm saying look at this file called requirements.text and again if you didn't want to have that file for whatever reason you can also just list them in this file here as well and then finally console scripts this is the bit which actually makes it into a command line tool and here we say I want to call my tool my awesome tool and I find what when I someone types that into the command line what I want python to do is to find the module called my tool which we've created here with the init file and I've actually got this script called analysis here again this file name could be whatever you want and then look for a function called app and then here I've or sorry a variable called here our variable is called but I could also put a function name and stuff here as well if I wanted the type I'm going to say.app okay so now python will know what to do when I install my tool and moment of truth let's try and install it and see what breaks so pip python package index uses pip and I'm going to say pip install now I could just do full stop for the current environment so my current working directory and that will work but I'm actually going to add minus e flag here but editable what that does is instead of copying all the files over to my python installation directory it soft links them and that's really useful when developing locally because I can make edits to this file hit save and adapt the reinstall tool every single time so I just am always in the habit of using minus e pretty much all the time and then let's see what happens yeah it broke setup not found that's because I got the import wrong right sorry from setup tools import setup and then set up search and I could have done set up like that that should work as well let's try again great you can see it's running through all those requirements it's installing all the back end stuff which is like matplotlib and and "typer" and stuff and it installed so now what did I call it my awesome tool if I do my awesome tool --help hooray it works like that we've got command line tool and now I can run this wherever I am on my system I don't have to be in this working directory anymore doesn't matter if I make do an example make the testing to do testing and then if I could do my awesome tool this is a test there we go now we've got that file created in there because that was my working directory and sure enough got a nice title brilliant okay so we have a command line tool it installs locally it works and it's got a nice command line interface we're nearly there the final thing then is to take this code and put it publish it put it onto the python package index now again if you start digging around on google you will find instructions on how to do this and it will say run a whole load of command line functions run those do this and that will publish it and there's like a sandbox environment where you can test first and you have to sign up to pi pi obviously and register and create a project and everything but what my recommendation is to keep things simple and it is the only way I do it now is to do all of this through github actions and automate your public public automate the publication of your package and that's all I'm going to show you today because I can I can walk you through that quite easily and it's the same logic so if you've not used github actions before the way it works is you create a directory called github.Github so it's a hidden directory and a subdirectory called workflows and then in here I'm going to create a new file which can be called anything deploy.Py.Yaml and then I'm going to cheat and copy because otherwise it's going to take me a while to type all this in but I'm going to walk you through it so this is yaml that tells github actions what to run and when to run it we have a name up here which can be anything and firstly we have a trigger and this tells github run this github action whenever this repository has a release and the the event type is published so whenever you create a new release on github and you click publish this workflow will run and it'll run on a default branch and then we have the actual the meat of it what's that what is it actually doing it's running on ubuntu it's checking out the source code first and setting up python now I install the dependencies manually here I'm not totally sure if this is actually required or not but it was in the last github actions I did so I thought I'd do it again first command is just upgrading pip itself and setting up setup tools and stuff and then we do the pip install.command again just to install whatever's in the current working directory so now on github actions your tool is installed and then we run this python command with setup.Py which is just calling setup tools and saying s dist so setup tools distribution and create a b dist we don't need to know what that means or why it's there but that's just the files that the python package index needs so now it's built the distribution locally and then finally we publish it to you can see where I copied it from we publish it to the python package index this is a check just to make sure if anyone has forked your repository don't bother trying to do this because it obviously won't work so I usually just put this in check if your github repository is called whatever and then use this python package index action which is a github action that someone else has written and with and here I'm using a password and this is a github action sequence and this is an api token that you can get from the python package index website when you're logged in and that gives you gives the github actions or the credentials it needs to be able to publish the python package for you and that's it if everything works well you stick all this on github you make it all lovely you hit release and then you will be able to watch that workflow running and it will say workflow published remember to change this version when you run it more than once because if you try and publish the same package twice with the same version number on python package index it will fail and as long as you bump that then everything should work and you should end up with a package on on pipy and when you have that package you'll be able to do name that's I think that's what python package index uses so you'll be able to pip install my tool from anywhere anyone will be able to do that and it will just work and that's it and then at that point you can pat yourself on your back think how amazing the job you've just done is and how anyone can now use your analysis tools prepare yourself an unsorted bug reports to github and take the next step and scaffold that pipy recipe into bioconda and do all the last stuff but like I say that's in a different talk and so I'm not going to swamp everyone by talking about that too much today right hopefully that made sense for everybody shout if you have any questions and I'd love to hear what workflows other people have and whether I made a mistake and you think I should do it in a different way and if your way is better thank you so much it's nice to see how some of the magic actually happens in the background so do we have any questions from the audience I've got one have you um have you tried cookie cutter and to automate all of this yeah I when I was prepping this with like five minutes to go I was desperately trying to find a link for a really nice project which I've seen and I've spoken to the authors and I cannot remember the name of it um and uh it's but there's a few of them floating around but there's one definitely for bioinformatics where you can do use a cookie cutter project and it creates scaffolds an entire python package index project for you with all of this stuff in place and it's probably much better and quicker but um I purposefully chose not to show that thing today because I was thinking of going from someone who already has a script which is working through and trying to explain what all the different stuff is doing but yeah if you're starting from scratch I would absolutely do that and if anyone has any good links for projects or can remember the projects I'm talking about please post them here or in slack and so yeah I'll just drop the link in the chat if someone don't know what we're talking about so it's about last but that links for cookie cutter itself right which is just like a generic templating tool yes um there are cookie cutter projects which people have created like template repositories specifically for python um if that makes sense we do have another question in the chat um someone is asking why not pyprojects.Toml yeah so this is something else I was debating on the start so this is a bit of history here when I started creating my first python projects you always used that setup.Py file and you still can um and it's a bit like how Nextflow config files are just a groovy script where you can do whatever you like setup.Py is the same it's just a python script where you can do whatever you like which is wonderful and horrifying so slowly over the last like python community moves slowly so for the last many years uh there's been a move away from that way of doing things into more standardized file types and there are two which are being used there's a setup.Config file which is exactly the same thing but in a structured file format um and the other one is pyproject.Toml which is the newer and and better way of doing things pyproject.Toml is nice because it's also um a standard for many other python uh tools with configs so if you want to use black to lint your code which you should because black is amazing you'll put your settings in pyproject.Toml if you use i don't know mypy for type hinting or any of these flake8 tools or whatever and it will be linting tools and stuff they all stick their settings in pyproject.Toml which is great because you have one config file for everything to do with your python project which is much nicer and you can also do all of your setup setup tools python stuff in there there are a couple of things which i found i think i'm missing correct me if i'm wrong i don't think you can point it to a requirements.Txt file for all requirements and it's quite useful having that file sometimes um maybe it doesn't matter uh and it's also i think setup tools website says it's like in beta and it might change so i thought i'd play it safe today and go for setup.Cfg which is newish but fairly safe um and uh but yeah pyproject.Toml is if you can make it work for you is probably a nicer way to do it we have some more comments so um there was a link posted to morris' cookie cutter package which has not been tried out at least not by the person who posted it and it says ironically flake8 can't actually work with settings from pyproject.Toml or at least couldn't a couple of months ago okay uh yeah so cookie cutter this might look familiar to anyone who's used the nf-core template we used to use cookie cutter for nf-core back in the early days and still use the underlying framework which is called ginger so that's where this double squiggly brackets comes from it's a templating system and you can see and here you've got all these different settings uh therefore with license options and a name and stuff and then these will go into all these double bracket things so the idea is you do cookie cutter run uh or cookie cutter i can't remember what the command is now build then you give it this github url and it will ask you a few questions which will just replace these defaults here um and then it will generate this package here but with all the template placeholders filled in great do we have any more questions it's doesn't seem so so thank you very much for this great talk on the slack channel yes before we wrap this up entirely i also have a um something to mention so next week's bytesized talk is going to be one hour late i will also post this again in the bytesized channel and very interestingly this will be um from a there will be a talk from people that were part of the mentorship program so the the deadline for the mentorship program program just got extended so it's actually for anyone who is still questioning if they should join or not this is your chance to actually listening to people who have been part of it and they give some impressions um so with this i would like to thank phil again i would like to thank everyone who listened again and um of course as usual i would like to thank the Chan Zuckerberg Initiative for funding our talks and have a great week everyone.

</details>
