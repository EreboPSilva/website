---
title: Basic training to create an nf-core pipeline
subtitle: A guide to create Nextflow pipelines using nf-core tools
---

# Introduction

## Objectives / Overview (which is better?)

## Prerequisites

## Follow the training videos

This training can be followed either based on this documentation alone, or via a training video hosted on youtube. You can find the youtube video in the Youtube playlist below:

(no such video yet)

# Using gitpod

For this tutorial we are going to use Gitpod, which is best for first-timers as this platform contains all the programs and data required.
Gitpod will contain a preconfigured Nextflow development environment and has the following requirements:

- A GitHub account
- Web browser (Google Chrome, Firefox)
- Internet connection

Simply click the link and log in using your GitHub account to start the tutorial:

<p class="text-center">
  <a href="https://www.gitpod.io/#https://github.com/nf-core/basic_training" class="btn btn-lg btn-success" target="_blank">
    Launch GitPod
  </a>
</p>

For more information about gitpod, including how to make your own gitpod environement, see the gitpod bytesize talk on youtube (link to the bytesize talk)

## Explore your Gitpod interface

You should now see something similar to the following:

(insert Gitpod welcome image)

- **The sidebar** allows you to customize your Gitpod environment and perform basic tasks (copy, paste, open files, search, git, etc.). Click the Explorer button to see which files are in this repository.
- **The terminal** allows you to run all the programs in the repository. For example, both `nextflow` and `docker` are installed and can be executed.
- **The main window** allows you to view and edit files. Clicking on a file in the explorer will open it within the main window. You should also see the nf-training material browser (<https://training.nextflow.io/>).

To test that the environment is working correctly, type the following into the terminal:

```bash
nextflow info
```

This should come up with the Nextflow version and runtime information:

```
Version: 22.10.4 build 5836
Created: 09-12-2022 09:58 UTC
System: Linux 5.15.0-47-generic
Runtime: Groovy 3.0.13 on OpenJDK 64-Bit Server VM 17.0.3-internal+0-adhoc..src
Encoding: UTF-8 (UTF-8)
```

## Reopening a Gitpod session

You can reopen an environment from <https://gitpod.io/workspaces>. Find your previous environment in the list, then select the ellipsis (three dots icon) and select Open.

If you have saved the URL for your previous Gitpod environment, you can simply open it in your browser.

Alternatively, you can start a new workspace by following the Gitpod URL: <https://gitpod.io/#https://github.com/nextflow-io/training>

If you have lost your environment, you can find the main scripts used in this tutorial in the `nf-training` directory.

## Saving files from Gitpod to your local machine

To save any file from the explorer panel, right-click the file and select Download.

# Explore nf-core/tools

The nf-core/tools package is already installed in the gitpod environment. Now you can check out which pipelines, subworkflows and modules are available via tools.

# Create a pipeline from template

To get started with your new pipeline, run the create command:

```
nf-core create
```

Although you can provide options on the command line, it’s easiest to use the interactive prompts.

### Pipeline git repo

The nf-core create command has made a fully fledged pipeline for you. Before getting too carried away looking at all of the files, note that it has also initiated a git repository:

```
git status
```

It’s actually created three branches for you:

```
git branch
```

Each have the same initial commit, with the vanilla template:

```
git log
```

This is important, because this shared git history with unmodified nf-core template in the TEMPLATE branch is how the nf-core automated template synchronisation works (see the docs for more details).

The main thing to remember with this is that:

When creating a new repository on https://github.com or equivalent, don’t initialise it - leave it bare and push everything from your local clone
Develop your code on either the master or dev branches and leave TEMPLATE alone.

## Run the new pipeline

The new pipeline should run with Nextflow, right out of the box. Let’s try:

```
cd ../
nextflow run nf-core-demo/ -profile test,docker --outdir test_results
```

This basic template pipeline contains already the FastQC and MultiQC modules, which do run on a selection of test data.

## Template code walk through

## Customising the template

In many of the files generated by the nf-core template, you’ll find code comments that look like this:

```
// TODO nf-core: Do something here
```

These are markers to help you get started with customising the template code as you write your pipeline. Editor tools such as Todo tree help you easily navigate these and work your way through them.

## Linting your pipeline

Customising the template is part of writing your new pipeline. However, not all files should be edited - indeed, nf-core strives to promote standardisation amongst pipelines.

To try to keep pipelines up to date and using the same code where possible, we have an automated code linting tool for nf-core pipelines. Running nf-core lint will run a comprehensive test suite against your pipeline:

```
cd nf-core-demo/
nf-core lint
```

Linting tests can have one of four statuses: pass, ignore, warn or fail. For example, at first you will see a large number of warnings about TODO comments, letting you know that you haven’t finished setting up your new pipeline. Warnings are ok at this stage, but should be cleared up before a pipeline release.

Failures are more serious however, and will typically prevent pull-requests from being merged. For example, if you edit CODE_OF_CONDUCT.md, which should match the template, you’ll get a pipeline lint test failure:

```
echo "Edited" >> CODE_OF_CONDUCT.md
nf-core lint
```

## Adding Modules to a pipeline

A module is a  single `process` built to be reusable and self-contained so it can be used within different Nextflow pipelines. They encapsulate a specific function or task, for example running a single tool such as [`FastQC`](https://github.com/nf-core/modules/blob/master/modules/nf-core/fastqc/main.nf). You can import and use modules like functions in a Nextflow subworkflow, this makes your workflow more readable and maintainable.

In nf-core modules are also standarised, i.e. they follow certain rules to optimise reusability  and compatibility between pipelines. Each module consists of two files: a `main.nf` script containing the module's code and a `meta.yml` file that provides general information about the module and defines its inputs and outputs, although this last one can be optional. You might also find other optional files such as `environment.yml`, which contains the packages to be installed with `conda` and a folder called `tests`, which contains the necessary files to perform validation on the module with [`nf-test`](https://code.askimed.com/nf-test/).

You can find a list of nf-core modules available in the [`modules/`](https://github.com/nf-core/modules/tree/master/modules) directory of nf-core/modules along with the required documentation and tests.

## Adding an existing nf-core module

### Identify available nf-core modules

The nf-core pipeline template comes with a few nf-core/modules pre-installed. You can list these with the command below:

```
nf-core modules list local
```

These version hashes and repository information for the source of the modules are tracked in the modules.json file in the root of the repo. This file will automatically be updated by nf-core/tools when you create, remove, update modules.

Let’s see if all of our modules are up-to-date:

```
nf-core modules update
```

You can list all of the modules available on nf-core/modules via the command below but we have added search functionality to the nf-core website to do this too!

```
nf-core modules list remote
```

### Install a remote nf-core module

To install a remote nf-core module from the website, you can first get information about a tool, including the installation command by executing:

```
nf-core modules info [salmon?]
```

Then you can execute the nf-core/tools installation command:

```
nf-core modules install [salmon?]
```

(lots of steps missing here
exercise to add a different module would be nice!
comparison to simple nextflow pipeline from the basic Nextflow training would be nice!)

## Adding a remote module

If there is no nf-core module available for the software you want to include, the nf-core tools package can also aid in the generation of a remote module that is specific for your pipeline. To add a remote module run the following:

```
nf-core modules create
```

Open ./modules/local/demo/module.nf and start customising this to your needs whilst working your way through the extensive TODO comments!

### Making a remote module for a custom script

To generate a module for a custom script you need to follow the same steps when adding a remote module.
Then, you can supply the command for your script in the `script` block but your script needs to be present
and *executable* in the `bin`
folder of the pipeline.
In the nf-core pipelines,
this folder is in the main directory and you can see in [`rnaseq`](https://github.com/nf-core/rnaseq).
Let's look at an publicly available example in this pipeline,
for instance [`tximport.r`](https://github.com/nf-core/rnaseq/blob/master/bin/tximport.r).
This is an Rscript present in the [`bin`](https://github.com/nf-core/rnaseq/tree/master/bin) of the pipeline.
We can find the module that runs this script in
[`modules/local/tximport`](https://github.com/nf-core/rnaseq/blob/master/modules/local/tximport/main.nf).
As we can see the script is being called in the `script` block, note that `tximport.r` is
being executed as if it was called from the command line and therefore needs to be *executable*.

<blockquote style="border-left: 4px solid #F0AD4E; background-color: #FFF3CD; padding: 10px;">

<h4 style="margin-top: 0;">TL;TR</h4>

1. Write your script on any language (python, bash, R,
   ruby). E.g. `maf2bed.py`
2. If not there yet, move your script to `bin` folder of
   the pipeline and make it
   executable (`chmod +x <filename>`)
3. Create a module with a single process to call your script from within the workflow. E.g. `./modules/local/convert_maf2bed/main.nf`
4. Include your new module in your workflow with the command `include {CONVERT_MAF2BED} from './modules/local/convert_maf2bed/main'` that is written before the workflow call.
</blockquote>

_Tip: Try to follow best practices when writing a script for
   reproducibility and maintenance purposes: add the
   shebang (e.g. `#!/usr/bin/env python`), and a header
   with description and type of license._

### 1. Write your script
Let's create a simple custom script that converts a MAF file to a BED file called `maf2bed.py` and place it in the bin directory of our nf-core-testpipeline::

```
#!/usr/bin/env python
"""bash title="maf2bed.py"
Author: Raquel Manzano - @RaqManzano
Script: Convert MAF to BED format keeping ref and alt info
License: MIT
"""
import argparse
import pandas as pd


def argparser():
    parser = argparse.ArgumentParser(description="")
    parser.add_argument("-maf", "--mafin", help="MAF input file", required=True)
    parser.add_argument("-bed", "--bedout", help="BED input file", required=True)
    parser.add_argument(
        "--extra", help="Extra columns to keep (space separated list)", nargs="+", required=False, default=[]
    )
    return parser.parse_args()

def maf2bed(maf_file, bed_file, extra):
    maf = pd.read_csv(maf_file, sep="\t", comment="#")
    bed = maf[["Chromosome", "Start_Position", "End_Position"] + extra]
    bed.to_csv(bed_file, sep="\t", index=False, header=False)


def main():
    args = argparser()
    maf2bed(maf_file=args.mafin, bed_file=args.bedout, extra=args.extra)


if __name__ == "__main__":
    main()

```

### 2. Make sure your script is in the right folder
Now, let's move it to the correct directory:

```
mv maf2bed.py /path/where/pipeline/is/bin/.
chmod +x /path/where/pipeline/is/bin/maf2bed.py
```

### 3. Create your custom module
Then, let's write our module. We will call the process
"CONVERT_MAF2BED" and add any tags or/and labels that
are appropriate (this is optional) and directives (via
conda and/or container) for
the definition of dependencies.


<details>
<summary><span style="color: forestgreen; font-weight: bold;">More info on labels</span></summary>
A `label` will
annotate the processes with a reusable identifier of your
choice that can be used for configuring. E.g. we use the
`label` 'process_single', this looks as follows:

```
withLabel:process_single {
        cpus   = { check_max( 1     * task.attempt, 'cpus'  ) }
        memory = { check_max( 1.GB  * task.attempt, 'memory') }
        time   = { check_max( 1.h   * task.attempt, 'time'  ) }
    }
```
</details>

<details>
<summary><span style="color: forestgreen; font-weight: bold;">More info on tags</span></summary>

A `tag` is simple a user provided identifier associated to
the task. In our process example, the input is a tuple
comprising a hash of metadata for the maf file called
`meta` and the path to the `maf` file. It may look
similar to: `[[id:'123', data_type:'maf'],
/path/to/file/example.maf]`. Hence, when nextflow makes
the call and `$meta.id` is `123` name of the job
will be "CONVERT_MAF2BED(123)". If `meta` does not have
`id` in its hash, then this will be literally `null`.

</details>

<details>
<summary><span style="color: forestgreen; font-weight: bold;">More info on conda/container directives</span></summary>

The `conda` directive allows for the definition of the
process dependencies using the [Conda package manager](https://docs.conda.io/en/latest/). Nextflow automatically sets up an environment for the given package names listed by in the conda directive. For example:

```
process foo {
  conda 'bwa=0.7.15'

  '''
  your_command --here
  '''
}
```
Multiple packages can be specified separating them with a blank space e.g. `bwa=0.7.15 samtools=1.15.1`. The name of the channel from where a specific package needs to be downloaded can be specified using the usual Conda notation i.e. prefixing the package with the channel name as shown here `bioconda::bwa=0.7.15`

```
process foo {
  conda 'bioconda::bwa=0.7.15 bioconda::samtools=1.15.1'

  '''
  your_bwa_cmd      --here
  your_samtools_cmd --here
  '''
}
```
Similarly, we can apply the `container` directive to execute the process script in a [Docker](http://docker.io/) or [Singularity](https://docs.sylabs.io/guides/3.5/user-guide/introduction.html) container. When running Docker, it requires the Docker daemon to be running in machine where the pipeline is executed, i.e. the local machine when using the local executor or the cluster nodes when the pipeline is deployed through a grid executor.

```
process foo {
  conda 'bioconda::bwa=0.7.15 bioconda::samtools=1.15.1'
  container 'dockerbox:tag'


  '''
  your_bwa_cmd      --here
  your_samtools_cmd --here
  '''
}
```

Additionally, the `container` directive allows for a more sophisticated choice of container and if it Docker or Singularity depending on the users choice of container engine. This practice is quite common on official nf-core modules.

```
process foo {
  conda "bioconda::fastqc=0.11.9"
  container "${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?
        'https://depot.galaxyproject.org/singularity/fastqc:0.11.9--0' :
        'biocontainers/fastqc:0.11.9--0' }"

  '''
  your_fastqc_command --here
  '''
}
```
</details>

Since `maf2bed.py` is in the `bin` directory we can directory call it in the script block of our new module `CONVERT_MAF2BED`. You only have to be careful with how you call variables (some explanations on when to use `${variable}` vs. `$variable`):
A process may contain any of the following definition blocks: directives, inputs, outputs, when clause, and the process script. Here is how we write it:

```
process CONVERT_MAF2BED {
    // HEADER
    tag "$meta.id"
    label 'process_single'
    // DEPENDENCIES DIRECTIVES
    conda "anaconda::pandas=1.4.3"
    container "${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?
        'https://depot.galaxyproject.org/singularity/pandas:1.4.3' :
        'quay.io/biocontainers/pandas:1.4.3' }"
    // INPUT BLOCK
    input:
    tuple val(meta), path(maf)
    // OUTPUT BLOCK
    output:
    tuple val(meta), path('*.bed') , emit: bed
    path "versions.yml"            , emit: versions
    // WHEN CLAUSE
    when:
    task.ext.when == null || task.ext.when
    // SCRIPT BLOCK
    script: // This script is bundled with the pipeline in bin
    def args = task.ext.args ?: ''
    def prefix = task.ext.prefix ?: "${meta.id}"

    """
maf2bed.py --mafin $maf --bedout ${prefix}.bed
    """
```

More on nextflow's process components in the [docs](https://www.nextflow.io/docs/latest/process.html).



### Include your module in the workflow
In general, we will call out nextflow module `main.nf` and save it in the `modules` folder under another folder called `conver_maf2bed`. If you believe your custom script could be useful for others and it is potentially reusable or calling a tool that is not yet present in nf-core modules you can start the process of making it official adding a `meta.yml` [explained above](#adding-modules-to-a-pipeline). In the `meta.yml` The overall tree for the pipeline skeleton will look as follows:

```
pipeline/
├── bin/
│   └── maf2bed.py
├── modules/
│   ├── local/
│   │   └── convert_maf2bed/
│   │       ├── main.nf
│   │       └── meta.yml
│   └── nf-core/
├── config/
│   ├── base.config
│   └── modules.config
...
```

To use our custom module located in `./modules/local/convert_maf2bed` within our workflow, we use a module inclusions command  as follows (this has to be done before we invoke our workflow):

```
include { CONVERT_MAF2BED } from './modules/local/convert_maf2bed/main'
workflow {
    input_data = [[id:123, data_type='maf'], /path/to/maf/example.maf]
    CONVERT_MAF2BED(input_data)
}
```

### Other notes
#### What happens in I want to use containers but there is no image created with the packages I need?

No worries, this can be done fairly easy thanks to [BioContainers](https://biocontainers-edu.readthedocs.io/en/latest/what_is_biocontainers.html), see instructions [here](https://github.com/BioContainers/multi-package-containers). If you see the combination that you need in the repo, you can also use [this website](https://midnighter.github.io/mulled) to find out the "mulled" name of this container.

### I want to know more about software dependencies!

You are in luck, we have more documentation [here](https://nf-co.re/docs/contributing/modules#software-requirements)


#### I want to know more about modules!
See more info about modules in the nextflow docs [here](https://nf-co.re/docs/contributing/modules#software-requirements.)


## Lint all modules

As well as the pipeline template you can lint individual or all modules with a single command:

```
nf-core modules lint --all
```

# Nextflow Schema

All nf-core pipelines can be run with --help to see usage instructions. We can try this with the demo pipeline that we just created:

```
cd ../
nextflow run nf-core-demo/ --help
```

## Working with Nextflow schema

If you peek inside the nextflow_schema.json file you will see that it is quite an intimidating thing. The file is large and complex, and very easy to break if edited manually.

Thankfully, we provide a user-friendly tool for editing this file: nf-core schema build.

To see this in action, let’s add some new parameters to nextflow.config:

```
params {
    demo                       = 'param-value-default'
    foo                        = null
    bar                        = false
    baz                        = 12
    // rest of the config file..

```

Then run nf-core schema build:

```
cd nf-core-demo/
nf-core schema build
```

The CLI tool should then prompt you to add each new parameter.
Here in the schema editor you can edit:

- Description and help text
- Type (string / boolean / integer etc)
- Grouping of parameters
- Whether a parameter is required, or hidden from help by default
- Enumerated values (choose from a list)
- Min / max values for numeric types
- Regular expressions for validation
- Special formats for strings, such as file-path
- Additional fields for files such as mime-type
